---
title: "Comparative Analysis of Deep Learning Models (Transformer and CodeBERT) for Program Translation"
title-block-banner: "	#54930E"
date: "7/10/2022"
categories:
  - research
  - python
  - dissertation
  - msc
---
## Abstract
Program translation refers to the technical process of automatically converting the source codeof a computer program written in one programming language into an equivalent program inanother. Deep learning models like the transformer and Code Bidirectional EncoderRepresentations from Transformers (CodeBERT) models can be trained to perform suchprogram translation. This study compares the transformer model and the CodeBERT-basedencoder-decoder model on the program translation task. Specifically, it trains the 6 and12-layer models for 50 and 100 epochs to translate programs written in Java to Python andPython to Java.A total of 3133 Java-Python parallel programs were collected, and then the models weretrained using the preprocessed training data. To compare the models, the Bilingual EvaluationUnderstudy (BLEU) and CodeBLEU scores were calculated on the test dataset. Among different layered models, the transformer model with 6 layers trained for 50 epochs totranslate from Java to Python achieved the highest BLEU and CodeBLEU scores, with valuesof 0.2812 and 0.2802, respectively. Similarly, the transformer model with 6 layers trained for100 epochs to translate from Python to Java received the highest BLEU and CodeBLEUscores of 0.3891 and 0.4018, respectively.These results show that the transformer models perform better than the CodeBERT models. Also, the BLEU and CodeBLEU scores of the Java to Python and Python to Java translationmodels are different. 

**Keywords:** *Machine Translation, Program Translation, Transformer, Code BidirectionalEncoder Representations from Transformers (CodeBERT), Bilingual Evaluation Understudy(BLEU), Code Bilingual Evaluation Understudy (CodeBLEU)*

## Links 
- [Publication](https://doi.org/10.3126/nccsrj.v3i1.72334)
- [ResearchGate](http://dx.doi.org/10.13140/RG.2.2.20721.49768/1)
- [Poster Presentation](http://dx.doi.org/10.13140/RG.2.2.12332.88964)

